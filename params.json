{
  "name": "Microsoft academic graph",
  "tagline": "Citation recommendation of 80 Million papers using Graph DB(Neo-4J)",
  "body": "Hello folks! Today, I will share another cool project I had been working on during my semester. It's called \"CITATION RECOMMENDATION IN MICROSOFT ACADEMIC GRAPH\". \r\n\r\nCitations have always been the best way to know popularity of a paper and it's author(s). The more the number of citations, the more popular and widely accepted the paper is. Google scholar does a great job of keeping authors and their followers keeping abreast with the status of paper/author citations. \r\nSo, what is a citation? In it's most rudimentary form, citation is a reference to a published paper (and author subsequently) in a paper being ready to be published. It's done in order to give attributions to others work which have been used while writing the current paper. It's a great practice, but not always an easy one. For example, if you are writing a paper on Neural Networks, their is so much information out there that you will be overwhelmed if you had to decide what all papers you should cite (since you would be going through a lot of them). To make this task easier, comes \"CITATION RECOMMENDATION\". The idea is to provide papers that an author should cite in his current work. Simple! Right? Well...not so much. There's just too many papers out there to be recommended that only a good algorithm will be able to give the best answers with minimum information. We tried to build something like that with our tools and knowledge of data science & big data.\r\n\r\nTo build this recommendation system, we used a dataset provided by [Microsoft Academic Graph](http://research.microsoft.com/en-us/projects/mag/) initiative. Microsoft researchers have done a great job explaining how and why the data was curated in their [paper](http://www.www2015.it/documents/proceedings/companion/p243.pdf). The data (99GB uncompressed) exceeds 83 Million papers by 20 Million authors from all across the globe (literally). Now, even though the figures sound gargantuan, they really aren't in Big Data world. But, it's good enough to get us started. Anyways, I won't be able to use all the data on my personal machine (a powerful yet restricted Macbook Air). So, let's go ahead and start our ANALYSIS!\r\n\r\nAs the most revered data scientist say, 'know the data', the obvious first step (assuming it's clean data) would be to do 'Exploratory Data Analysis or EDA'. Sounds cool. Right? It indeed is, because this is where we see all the surprises that the data gives us- weird/missing values, strange patterns, seemingly impossible correlations and so on. \r\n\r\nThe EDA on our data gives us some really good results.\r\n\r\n![Word Cloud](https://raw.githubusercontent.com/abhie19/MS_Academic_Graph/master/Images/Wordcloud.png)\r\n\r\nThe above word cloud represents the keywords that have occurred most frequently in the dataset we had. For example, we can infer that there have been lots of papers referring to biological keywords (cells, analysis, protein, etc), while environmental sciences (water, environment,climate, etc) don't publish many papers. Thus, this gives us a pretty good idea of what we are dealing with (lots n lots of biology papers).\r\n\r\nNext, we see the effect of year of publishing on the number of citations.\r\n\r\n![Image 1800s](https://raw.githubusercontent.com/abhie19/MS_Academic_Graph/master/Images/1800s%20Cite.png)\r\n![Image 1950s](https://raw.githubusercontent.com/abhie19/MS_Academic_Graph/master/Images/1950s%20cite.png)\r\n![Image 2015s](https://raw.githubusercontent.com/abhie19/MS_Academic_Graph/master/Images/2015s%20Cite.png)\r\n\r\nAs we can see above, the papers from 1800s (yes they wrote back then as well) got extremely low citations, the one's in 1950s got moderate number of citations, but the most recent ones have already got tons of citations to their name. So, yes, \"Age is not just a number here\", year of publishing does have an impact on citation numbers.\r\n\r\nIn our data, we have something given as rank of paper (more details in Microsoft's paper). Let's see if it affects citation. It obviously should if you ask me. So lets see from analysis.\r\n\r\n-\r\nOkay, so my hypothesis holds true, the rank of papers do matter.\r\n\r\nIn order to take this to the next level, we created a regression model to check out the dependent and independent values, see correlations and find out which attributes of the paper would help us most during recommendation. Even though the initial regression model: \r\n> citationcount ~ papercnt + jrnl + confront\r\n rejected the null hypothesis with a small (& controversial) p-value, it did't prove really effective in the later models.\r\n\r\n_Note: This was all done with mongoDB as database & not graph DB Neo4J._\r\n\r\nNow that we have enough familiarity with the data, we could go ahead and start our experiments using a graphDB. For our purpose, we used Neo4J, the de-facto GraphDB currently. It has a community edition available (free!!) and lots of help on forums. Neo4J uses it's own query language called - \"CYPHER\" (cool name for sure). CYPHER is pretty easy to understand, use and has similarity with SQL. Neo4J has a very cool UI wherein you can put in your queries and see the results right on the webpage as interactive graph/table/JSON. The graphs look really neat and organized, but it comes at the cost of clutter and node restrictions, since you can only display (& interpret) certain number of nodes. Although, the number of nodes on the graph is not restricted, but once you go to 100+ nodes it becomes uninterpretable (& super slow). But, this doesn't kill the purpose of GraphDB in anyway, since you can still store million of nodes in the database and do your analysis by looking at nodes, properties and relationships as awesome graphs (yup, they look pretty).\r\n\r\nEvaluation:\r\n\r\nIn order to evaluate the performance of our recommendation system, we use Precision and Recall as the metric. For a given paper, we compare the existing citations with the ones recommended by our system. The details could be found in the report.\r\n\r\nConclusion:\r\n\r\nWe successfully completed the task of paper citation recommendation. We got the best accuracy of 18% when we varied the number of recommended citations to 60. We believe this result can be improved by considered additional features (especially paper abstract/content for topic modeling) provided in the dataset. \r\n\r\nHere's a demo video of the project.\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}